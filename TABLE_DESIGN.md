# MinIODBè¡¨æ¦‚å¿µæ‰©å±•è®¾è®¡æ–¹æ¡ˆ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†ä¸ºMinIODBé¡¹ç›®å¼•å…¥tableï¼ˆè¡¨ï¼‰æ¦‚å¿µçš„å®Œæ•´è®¾è®¡æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆæ—¨åœ¨å°†å½“å‰æ··åˆå­˜å‚¨çš„æ•°æ®æŒ‰ä¸šåŠ¡ç±»å‹è¿›è¡Œåˆ†ç¦»ï¼Œæä¾›è¡¨çº§çš„ç®¡ç†ã€é…ç½®å’Œæƒé™æ§åˆ¶èƒ½åŠ›ã€‚

## ğŸ” å½“å‰æ¶æ„åˆ†æ

### ç°æœ‰æ•°æ®ç»„ç»‡æ–¹å¼
- **å­˜å‚¨è·¯å¾„**ï¼š`bucket/ID/YYYY-MM-DD/timestamp.parquet`
- **Redisç´¢å¼•**ï¼š`index:id:{ID}:{YYYY-MM-DD}`
- **ç¼“å†²åŒºé”®**ï¼š`{ID}/{YYYY-MM-DD}`
- **æ•°æ®ç»“æ„**ï¼šç»Ÿä¸€çš„DataRowï¼ˆIDã€Timestampã€Payloadï¼‰

### å­˜åœ¨çš„é—®é¢˜

1. **æ•°æ®æ··åˆå­˜å‚¨**ï¼šæ‰€æœ‰ç±»å‹çš„æ•°æ®å­˜å‚¨åœ¨åŒä¸€ä¸ªå‘½åç©ºé—´
2. **æ— ä¸šåŠ¡åˆ†ç¦»**ï¼šä¸åŒä¸šåŠ¡æ•°æ®æ— æ³•ç‹¬ç«‹ç®¡ç†
3. **æŸ¥è¯¢æ•ˆç‡ä½**ï¼šéœ€è¦æ‰«ææ‰€æœ‰æ•°æ®è¿›è¡Œè¿‡æ»¤
4. **é…ç½®å•ä¸€**ï¼šæ— æ³•ä¸ºä¸åŒç±»å‹æ•°æ®è®¾ç½®ä¸åŒç­–ç•¥
5. **æƒé™ç²—ç³™**ï¼šæ— æ³•å®ç°è¡¨çº§æƒé™æ§åˆ¶

## ğŸ¯ Tableæ¦‚å¿µè®¾è®¡æ–¹æ¡ˆ

### 1. æ ¸å¿ƒè®¾è®¡ç†å¿µ

**è¡¨çš„å®šä¹‰**ï¼š
- è¡¨æ˜¯æ•°æ®çš„é€»è¾‘åˆ†ç»„å•ä½
- æ¯ä¸ªè¡¨æ‹¥æœ‰ç‹¬ç«‹çš„å­˜å‚¨ç©ºé—´ã€ç´¢å¼•å’Œé…ç½®
- æ”¯æŒè¡¨çº§çš„æƒé™æ§åˆ¶å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†

**è®¾è®¡åŸåˆ™**ï¼š
- **å‘åå…¼å®¹**ï¼šç°æœ‰APIå’Œæ•°æ®ä¸å—å½±å“
- **æ¸è¿›å¼**ï¼šåˆ†é˜¶æ®µå®æ–½ï¼Œé™ä½é£é™©
- **é«˜æ€§èƒ½**ï¼šè¡¨çº§åˆ†ç¦»æå‡æŸ¥è¯¢æ•ˆç‡
- **æ˜“ç®¡ç†**ï¼šç®€åŒ–è¿ç»´å’Œç›‘æ§

### 2. æ•°æ®å­˜å‚¨å±‚æ”¹é€ 

#### å­˜å‚¨è·¯å¾„ç»“æ„
```
ç°æœ‰ï¼šbucket/ID/YYYY-MM-DD/timestamp.parquet
æ”¹ä¸ºï¼šbucket/TABLE/ID/YYYY-MM-DD/timestamp.parquet

ç¤ºä¾‹ï¼š
- olap-data/users/user-123/2024-01-15/1705123456789.parquet
- olap-data/orders/order-456/2024-01-15/1705123456790.parquet
- olap-data/logs/app-logs/2024-01-15/1705123456791.parquet
```

#### Redisç´¢å¼•ç»“æ„
```
ç°æœ‰ï¼šindex:id:{ID}:{YYYY-MM-DD}
æ”¹ä¸ºï¼šindex:table:{TABLE}:id:{ID}:{YYYY-MM-DD}

æ–°å¢ç´¢å¼•ï¼š
- tables:list                    # å­˜å‚¨æ‰€æœ‰è¡¨å (SET)
- table:{TABLE}:config          # è¡¨é…ç½®ä¿¡æ¯ (HASH)
- table:{TABLE}:stats           # è¡¨ç»Ÿè®¡ä¿¡æ¯ (HASH)
- table:{TABLE}:created_at      # è¡¨åˆ›å»ºæ—¶é—´ (STRING)
- table:{TABLE}:last_write      # æœ€åå†™å…¥æ—¶é—´ (STRING)
```

### 3. APIæ¥å£æ‰©å±•

#### ç°æœ‰æ¥å£æ”¹é€ 
```protobuf
// å†™å…¥æ¥å£ - æ·»åŠ tableå­—æ®µ
message WriteRequest {
  string table = 1;                    // æ–°å¢ï¼šè¡¨å
  string id = 2;                       // åŸæœ‰ï¼šè®°å½•ID
  google.protobuf.Timestamp timestamp = 3;  // åŸæœ‰ï¼šæ—¶é—´æˆ³
  google.protobuf.Struct payload = 4;  // åŸæœ‰ï¼šæ•°æ®è½½è·
}

// æŸ¥è¯¢æ¥å£ - æ”¯æŒè¡¨åè§£æ
message QueryRequest {
  string sql = 1;  // SQLä¸­ä½¿ç”¨å®é™…è¡¨åï¼Œå¦‚ "SELECT * FROM users WHERE id = 'user-123'"
}
```

#### æ–°å¢è¡¨ç®¡ç†æ¥å£
```protobuf
// åˆ›å»ºè¡¨
message CreateTableRequest {
  string table_name = 1;
  TableConfig config = 2;
  bool if_not_exists = 3;
}

message CreateTableResponse {
  bool success = 1;
  string message = 2;
}

// åˆ é™¤è¡¨
message DropTableRequest {
  string table_name = 1;
  bool if_exists = 2;
  bool cascade = 3;  // æ˜¯å¦çº§è”åˆ é™¤æ•°æ®
}

message DropTableResponse {
  bool success = 1;
  string message = 2;
  int32 files_deleted = 3;
}

// åˆ—å‡ºè¡¨
message ListTablesRequest {
  string pattern = 1;  // è¡¨åæ¨¡å¼åŒ¹é…
}

message ListTablesResponse {
  repeated TableInfo tables = 1;
  int32 total = 2;
}

// è¡¨æè¿°
message DescribeTableRequest {
  string table_name = 1;
}

message DescribeTableResponse {
  TableInfo table_info = 1;
  TableStats stats = 2;
}

// è¡¨ä¿¡æ¯
message TableInfo {
  string name = 1;
  TableConfig config = 2;
  string created_at = 3;
  string last_write = 4;
  string status = 5;  // active, archived, deleting
}

// è¡¨é…ç½®
message TableConfig {
  int32 buffer_size = 1;
  int32 flush_interval_seconds = 2;
  int32 retention_days = 3;
  bool backup_enabled = 4;
  map<string, string> properties = 5;
}

// è¡¨ç»Ÿè®¡
message TableStats {
  int64 record_count = 1;
  int64 file_count = 2;
  int64 size_bytes = 3;
  string oldest_record = 4;
  string newest_record = 5;
}
```

#### æœåŠ¡æ¥å£æ‰©å±•
```protobuf
service OlapService {
  // ç°æœ‰æ¥å£
  rpc Write (WriteRequest) returns (WriteResponse);
  rpc Query (QueryRequest) returns (QueryResponse);
  rpc TriggerBackup(TriggerBackupRequest) returns (TriggerBackupResponse);
  rpc RecoverData(RecoverDataRequest) returns (RecoverDataResponse);
  rpc HealthCheck(HealthCheckRequest) returns (HealthCheckResponse);
  rpc GetStats(GetStatsRequest) returns (GetStatsResponse);
  rpc GetNodes(GetNodesRequest) returns (GetNodesResponse);
  
  // æ–°å¢è¡¨ç®¡ç†æ¥å£
  rpc CreateTable(CreateTableRequest) returns (CreateTableResponse);
  rpc DropTable(DropTableRequest) returns (DropTableResponse);
  rpc ListTables(ListTablesRequest) returns (ListTablesResponse);
  rpc DescribeTable(DescribeTableRequest) returns (DescribeTableResponse);
}
```

### 4. ç¼“å†²åŒºç®¡ç†æ”¹é€ 

#### ç¼“å†²åŒºç»“æ„è°ƒæ•´
```go
// å½“å‰ç»“æ„
type SharedBuffer struct {
    buffer map[string][]DataRow  // key: "{ID}/{YYYY-MM-DD}"
    // ...
}

// æ”¹é€ åç»“æ„
type SharedBuffer struct {
    buffer map[string][]DataRow  // key: "{TABLE}/{ID}/{YYYY-MM-DD}"
    tableConfigs map[string]*TableConfig  // è¡¨çº§é…ç½®
    tableMutexes map[string]*sync.RWMutex // è¡¨çº§é”
    // ...
}

type TableConfig struct {
    BufferSize     int           `yaml:"buffer_size"`
    FlushInterval  time.Duration `yaml:"flush_interval"`
    RetentionDays  int           `yaml:"retention_days"`
    BackupEnabled  bool          `yaml:"backup_enabled"`
    Properties     map[string]string `yaml:"properties"`
}
```

#### è¡¨çº§åˆ·æ–°ç­–ç•¥
```go
// æŒ‰è¡¨è¿›è¡Œå·®å¼‚åŒ–åˆ·æ–°
func (b *SharedBuffer) shouldFlush(table string, bufferKey string) bool {
    config := b.getTableConfig(table)
    rows := b.buffer[bufferKey]
    return len(rows) >= config.BufferSize
}

// è·å–è¡¨é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
func (b *SharedBuffer) getTableConfig(table string) *TableConfig {
    if config, exists := b.tableConfigs[table]; exists {
        return config
    }
    return b.defaultConfig
}

// è¡¨çº§åˆ·æ–°ä»»åŠ¡
func (b *SharedBuffer) flushTable(table string) {
    config := b.getTableConfig(table)
    
    // è·å–è¯¥è¡¨çš„æ‰€æœ‰ç¼“å†²åŒºé”®
    tableKeys := b.getTableBufferKeys(table)
    
    for _, key := range tableKeys {
        if b.shouldFlush(table, key) {
            b.flushBuffer(key)
        }
    }
}
```

### 5. æŸ¥è¯¢å¼•æ“å¢å¼º

#### SQLè§£æå¢å¼º
```go
type QueryParser struct {
    redisClient *redis.Client
}

type QueryPlan struct {
    Tables    []string
    SQL       string
    Filters   map[string]*QueryFilter
}

func (p *QueryParser) ParseSQL(sql string) (*QueryPlan, error) {
    // 1. è§£æSQLï¼Œæå–è¡¨å
    tables, err := p.extractTableNames(sql)
    if err != nil {
        return nil, fmt.Errorf("failed to extract table names: %w", err)
    }
    
    // 2. éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
    for _, table := range tables {
        exists, err := p.tableExists(table)
        if err != nil {
            return nil, fmt.Errorf("failed to check table existence: %w", err)
        }
        if !exists {
            return nil, fmt.Errorf("table %s does not exist", table)
        }
    }
    
    // 3. è§£æWHEREæ¡ä»¶ï¼Œæå–è¿‡æ»¤å™¨
    filters := make(map[string]*QueryFilter)
    for _, table := range tables {
        filter, err := p.parseTableFilter(sql, table)
        if err != nil {
            return nil, fmt.Errorf("failed to parse filter for table %s: %w", table, err)
        }
        filters[table] = filter
    }
    
    return &QueryPlan{
        Tables:  tables,
        SQL:     sql,
        Filters: filters,
    }, nil
}

func (p *QueryParser) extractTableNames(sql string) ([]string, error) {
    // ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ–SQLè§£æå™¨æå–è¡¨å
    // æ”¯æŒ FROM table_name, JOIN table_name ç­‰è¯­æ³•
    re := regexp.MustCompile(`(?i)\b(?:FROM|JOIN)\s+([a-zA-Z_][a-zA-Z0-9_]*)`)
    matches := re.FindAllStringSubmatch(sql, -1)
    
    tables := make([]string, 0)
    seen := make(map[string]bool)
    
    for _, match := range matches {
        if len(match) > 1 {
            table := match[1]
            if !seen[table] {
                tables = append(tables, table)
                seen[table] = true
            }
        }
    }
    
    return tables, nil
}

func (p *QueryParser) tableExists(table string) (bool, error) {
    ctx := context.Background()
    return p.redisClient.SIsMember(ctx, "tables:list", table).Result()
}
```

#### å¤šè¡¨æŸ¥è¯¢æ”¯æŒ
```go
func (q *Querier) ExecuteMultiTableQuery(plan *QueryPlan) (string, error) {
    ctx := context.Background()
    
    // 1. ä¸ºæ¯ä¸ªè¡¨è·å–ç›¸å…³æ–‡ä»¶
    tableFiles := make(map[string][]string)
    for _, table := range plan.Tables {
        filter := plan.Filters[table]
        files, err := q.getTableFiles(ctx, table, filter)
        if err != nil {
            return "", fmt.Errorf("failed to get files for table %s: %w", table, err)
        }
        tableFiles[table] = files
    }
    
    // 2. åœ¨DuckDBä¸­åˆ›å»ºè™šæ‹Ÿè¡¨
    for table, files := range tableFiles {
        if len(files) == 0 {
            // åˆ›å»ºç©ºè¡¨
            viewSQL := fmt.Sprintf(
                "CREATE VIEW %s AS SELECT * FROM (VALUES (NULL, NULL, NULL)) AS t(id, timestamp, payload) WHERE false",
                table,
            )
        } else {
            viewSQL := fmt.Sprintf(
                "CREATE VIEW %s AS SELECT * FROM read_parquet(['%s'])",
                table,
                strings.Join(files, "','"),
            )
        }
        
        if _, err := q.db.Exec(viewSQL); err != nil {
            return "", fmt.Errorf("failed to create view for table %s: %w", table, err)
        }
        
        // æŸ¥è¯¢å®Œæˆåæ¸…ç†è§†å›¾
        defer func(tableName string) {
            q.db.Exec(fmt.Sprintf("DROP VIEW IF EXISTS %s", tableName))
        }(table)
    }
    
    // 3. æ‰§è¡ŒåŸå§‹SQL
    return q.executeSQL(plan.SQL)
}

func (q *Querier) getTableFiles(ctx context.Context, table string, filter *QueryFilter) ([]string, error) {
    var allFiles []string
    
    // æ„å»ºRedisé”®æ¨¡å¼
    var pattern string
    if filter.ID != "" && filter.Day != "" {
        // ç²¾ç¡®æŸ¥è¯¢
        redisKey := fmt.Sprintf("index:table:%s:id:%s:%s", table, filter.ID, filter.Day)
        files, err := q.redisClient.SMembers(ctx, redisKey).Result()
        if err != nil && err != redis.Nil {
            return nil, err
        }
        for _, file := range files {
            allFiles = append(allFiles, fmt.Sprintf("s3://olap-data/%s", file))
        }
    } else if filter.ID != "" {
        // æŒ‰IDæŸ¥è¯¢
        pattern = fmt.Sprintf("index:table:%s:id:%s:*", table, filter.ID)
    } else if filter.Day != "" {
        // æŒ‰æ—¥æœŸæŸ¥è¯¢
        pattern = fmt.Sprintf("index:table:%s:id:*:%s", table, filter.Day)
    } else {
        // å…¨è¡¨æ‰«æ
        pattern = fmt.Sprintf("index:table:%s:id:*", table)
    }
    
    if pattern != "" {
        keys, err := q.redisClient.Keys(ctx, pattern).Result()
        if err != nil {
            return nil, err
        }
        
        for _, key := range keys {
            files, err := q.redisClient.SMembers(ctx, key).Result()
            if err != nil {
                continue
            }
            for _, file := range files {
                allFiles = append(allFiles, fmt.Sprintf("s3://olap-data/%s", file))
            }
        }
    }
    
    // è·å–ç¼“å†²åŒºä¸­çš„æ•°æ®
    bufferFiles := q.getTableBufferFiles(table, filter)
    allFiles = append(allFiles, bufferFiles...)
    
    return allFiles, nil
}
```

### 6. é…ç½®ç®¡ç†æ‰©å±•

#### å…¨å±€é…ç½®
```yaml
# config.yaml
tables:
  # é»˜è®¤è¡¨é…ç½®
  default_config:
    buffer_size: 1000
    flush_interval: 30s
    retention_days: 365
    backup_enabled: true
  
  # è¡¨çº§é…ç½®è¦†ç›–
  users:
    buffer_size: 2000
    flush_interval: 60s
    retention_days: 2555  # 7å¹´
    backup_enabled: true
    properties:
      description: "ç”¨æˆ·æ•°æ®è¡¨"
      owner: "user-service"
  
  orders:
    buffer_size: 5000
    flush_interval: 10s
    retention_days: 2555
    backup_enabled: true
    properties:
      description: "è®¢å•æ•°æ®è¡¨"
      owner: "order-service"
  
  logs:
    buffer_size: 10000
    flush_interval: 5s
    retention_days: 30
    backup_enabled: false
    properties:
      description: "åº”ç”¨æ—¥å¿—è¡¨"
      owner: "log-service"

# è¡¨ç®¡ç†é…ç½®
table_management:
  auto_create_tables: true      # æ˜¯å¦è‡ªåŠ¨åˆ›å»ºè¡¨
  default_table: "default"      # é»˜è®¤è¡¨åï¼ˆå‘åå…¼å®¹ï¼‰
  max_tables: 1000             # æœ€å¤§è¡¨æ•°é‡é™åˆ¶
  table_name_pattern: "^[a-zA-Z][a-zA-Z0-9_]{0,63}$"  # è¡¨åè§„åˆ™
```

#### è¡¨çº§æƒé™é…ç½®
```yaml
# æƒé™é…ç½®ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
permissions:
  enabled: true
  
  # è§’è‰²å®šä¹‰
  roles:
    admin:
      - "table:*:*"           # æ‰€æœ‰è¡¨çš„æ‰€æœ‰æƒé™
    
    user_service:
      - "table:users:read"
      - "table:users:write"
      
    order_service:
      - "table:orders:read"
      - "table:orders:write"
      - "table:users:read"    # å¯ä»¥è¯»å–ç”¨æˆ·è¡¨
    
    analyst:
      - "table:*:read"        # æ‰€æœ‰è¡¨çš„è¯»æƒé™
      
    logger:
      - "table:logs:write"    # åªèƒ½å†™æ—¥å¿—è¡¨
  
  # ç”¨æˆ·è§’è‰²æ˜ å°„
  users:
    "service-user-001": ["user_service"]
    "service-order-001": ["order_service"]
    "analyst-001": ["analyst"]
    "admin-001": ["admin"]
```

### 7. å®æ–½ç­–ç•¥

#### åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

**ç¬¬ä¸€é˜¶æ®µï¼šAPIæ‰©å±•ï¼ˆ2å‘¨ï¼‰**
- [ ] æ‰©å±•protobufå®šä¹‰ï¼Œæ·»åŠ tableå­—æ®µå’Œè¡¨ç®¡ç†æ¥å£
- [ ] ä¿®æ”¹WriteRequestå¤„ç†ï¼Œæ”¯æŒtableå‚æ•°
- [ ] å®ç°åŸºæœ¬çš„è¡¨ç®¡ç†APIï¼ˆCreateTableã€ListTablesç­‰ï¼‰
- [ ] ä¿æŒå‘åå…¼å®¹ï¼ŒæœªæŒ‡å®štableæ—¶ä½¿ç”¨é»˜è®¤è¡¨
- [ ] æ·»åŠ è¡¨å­˜åœ¨æ€§éªŒè¯å’ŒåŸºæœ¬é…ç½®ç®¡ç†

**ç¬¬äºŒé˜¶æ®µï¼šå­˜å‚¨æ”¹é€ ï¼ˆ3å‘¨ï¼‰**
- [ ] ä¿®æ”¹ç¼“å†²åŒºç®¡ç†ï¼Œæ”¯æŒè¡¨çº§åˆ†ç¦»
- [ ] è°ƒæ•´æ–‡ä»¶å­˜å‚¨è·¯å¾„ç»“æ„
- [ ] æ›´æ–°Redisç´¢å¼•ç»“æ„
- [ ] å®ç°æ•°æ®è¿ç§»å·¥å…·
- [ ] æ·»åŠ è¡¨çº§åˆ·æ–°ç­–ç•¥å’Œé…ç½®

**ç¬¬ä¸‰é˜¶æ®µï¼šæŸ¥è¯¢å¢å¼ºï¼ˆ2å‘¨ï¼‰**
- [ ] å¢å¼ºSQLè§£æï¼Œæ”¯æŒè¡¨åæå–å’ŒéªŒè¯
- [ ] å®ç°å¤šè¡¨æŸ¥è¯¢æ”¯æŒ
- [ ] ä¼˜åŒ–å•è¡¨æŸ¥è¯¢æ€§èƒ½
- [ ] æ·»åŠ æŸ¥è¯¢ç»Ÿè®¡å’Œç›‘æ§
- [ ] å®ç°ç¼“å†²åŒºæ•°æ®çš„è¡¨çº§æŸ¥è¯¢

**ç¬¬å››é˜¶æ®µï¼šé«˜çº§åŠŸèƒ½ï¼ˆ3å‘¨ï¼‰**
- [ ] å®ç°è¡¨çº§æƒé™æ§åˆ¶
- [ ] æ·»åŠ è¡¨çº§ç›‘æ§å’Œå‘Šè­¦
- [ ] å®ç°è¡¨çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆè‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®ï¼‰
- [ ] ä¼˜åŒ–è·¨è¡¨æŸ¥è¯¢æ€§èƒ½
- [ ] æ·»åŠ è¡¨çº§å¤‡ä»½å’Œæ¢å¤åŠŸèƒ½

#### å‘åå…¼å®¹ç­–ç•¥

**APIå…¼å®¹æ€§**ï¼š
```go
// å¤„ç†æ²¡æœ‰tableå­—æ®µçš„æ—§è¯·æ±‚
func (s *OlapService) Write(ctx context.Context, req *WriteRequest) (*WriteResponse, error) {
    table := req.Table
    if table == "" {
        table = s.config.DefaultTable  // é»˜è®¤ä¸º "default"
    }
    
    // éªŒè¯è¡¨å
    if !s.isValidTableName(table) {
        return &WriteResponse{
            Success: false,
            Message: fmt.Sprintf("invalid table name: %s", table),
        }, nil
    }
    
    // è‡ªåŠ¨åˆ›å»ºè¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if s.config.AutoCreateTables {
        if err := s.ensureTableExists(ctx, table); err != nil {
            return &WriteResponse{
                Success: false,
                Message: fmt.Sprintf("failed to create table: %v", err),
            }, nil
        }
    }
    
    // ç»§ç»­å¤„ç†å†™å…¥é€»è¾‘...
    return s.writeToTable(ctx, table, req)
}

// æŸ¥è¯¢å…¼å®¹æ€§å¤„ç†
func (s *OlapService) Query(ctx context.Context, req *QueryRequest) (*QueryResponse, error) {
    // æ£€æŸ¥SQLä¸­æ˜¯å¦ä½¿ç”¨äº†æ—§çš„"table"å…³é”®å­—
    if strings.Contains(strings.ToLower(req.Sql), "from table") {
        // æ›¿æ¢ä¸ºé»˜è®¤è¡¨å
        req.Sql = strings.ReplaceAll(
            req.Sql, 
            "FROM table", 
            fmt.Sprintf("FROM %s", s.config.DefaultTable),
        )
        req.Sql = strings.ReplaceAll(
            req.Sql, 
            "from table", 
            fmt.Sprintf("from %s", s.config.DefaultTable),
        )
    }
    
    // ç»§ç»­å¤„ç†æŸ¥è¯¢é€»è¾‘...
    return s.executeQuery(ctx, req)
}
```

**æ•°æ®è¿ç§»**ï¼š
```bash
# æ•°æ®è¿ç§»å·¥å…·
./miniodb migrate \
  --from-pattern "index:id:*" \
  --to-table "default" \
  --dry-run

# å®é™…è¿ç§»
./miniodb migrate \
  --from-pattern "index:id:*" \
  --to-table "default" \
  --batch-size 1000

# éªŒè¯è¿ç§»ç»“æœ
./miniodb migrate \
  --verify \
  --table "default"
```

**é…ç½®è¿ç§»**ï¼š
```yaml
# æ—§é…ç½®è‡ªåŠ¨æ˜ å°„åˆ°é»˜è®¤è¡¨
buffer:
  buffer_size: 1000
  flush_interval: 30s

# è‡ªåŠ¨è½¬æ¢ä¸º
tables:
  default_config:
    buffer_size: 1000
    flush_interval: 30s
    retention_days: 365
    backup_enabled: true
```

### 8. ç›‘æ§å’Œè¿ç»´

#### è¡¨çº§ç›‘æ§æŒ‡æ ‡
```yaml
# PrometheusæŒ‡æ ‡
miniodb_table_records_total{table="users"}              # è¡¨è®°å½•æ€»æ•°
miniodb_table_files_total{table="users"}                # è¡¨æ–‡ä»¶æ€»æ•°
miniodb_table_size_bytes{table="users"}                 # è¡¨å­˜å‚¨å¤§å°
miniodb_table_buffer_size{table="users"}                # è¡¨ç¼“å†²åŒºå¤§å°
miniodb_table_buffer_pending{table="users"}             # è¡¨ç¼“å†²åŒºå¾…å†™å…¥æ•°æ®
miniodb_table_query_duration_seconds{table="users"}     # è¡¨æŸ¥è¯¢è€—æ—¶
miniodb_table_write_duration_seconds{table="users"}     # è¡¨å†™å…¥è€—æ—¶
miniodb_table_flush_duration_seconds{table="users"}     # è¡¨åˆ·æ–°è€—æ—¶
miniodb_table_last_write_timestamp{table="users"}       # è¡¨æœ€åå†™å…¥æ—¶é—´
miniodb_table_last_flush_timestamp{table="users"}       # è¡¨æœ€ååˆ·æ–°æ—¶é—´
```

#### è¿ç»´å·¥å…·
```bash
# è¡¨ç®¡ç†å‘½ä»¤
./miniodb table create users --buffer-size=2000 --retention-days=365
./miniodb table list
./miniodb table list --pattern="user*"
./miniodb table describe users
./miniodb table stats users
./miniodb table config users --buffer-size=3000
./miniodb table drop logs --cascade

# æ•°æ®ç®¡ç†å‘½ä»¤
./miniodb data count --table=users
./miniodb data size --table=users
./miniodb data cleanup --table=users --older-than=30d
./miniodb data backup --table=users
./miniodb data restore --table=users --from-backup=backup-20240115

# æŸ¥è¯¢å·¥å…·
./miniodb query "SELECT COUNT(*) FROM users WHERE id LIKE 'user-%'"
./miniodb query "SELECT u.name, COUNT(o.id) FROM users u LEFT JOIN orders o ON u.id = o.user_id GROUP BY u.name"

# ç›‘æ§å‘½ä»¤
./miniodb monitor tables
./miniodb monitor table users
./miniodb health --table=users
```

#### æ—¥å¿—å’Œå®¡è®¡
```go
// è¡¨çº§æ“ä½œæ—¥å¿—
type TableOperation struct {
    Timestamp   time.Time `json:"timestamp"`
    Table       string    `json:"table"`
    Operation   string    `json:"operation"`  // create, drop, write, query
    User        string    `json:"user"`
    Details     string    `json:"details"`
    Duration    int64     `json:"duration_ms"`
    RecordCount int64     `json:"record_count,omitempty"`
    Success     bool      `json:"success"`
    Error       string    `json:"error,omitempty"`
}

// å®¡è®¡æ—¥å¿—ç¤ºä¾‹
{
  "timestamp": "2024-01-15T10:30:00Z",
  "table": "users",
  "operation": "write",
  "user": "service-user-001",
  "details": "batch write 500 records",
  "duration_ms": 150,
  "record_count": 500,
  "success": true
}
```

### 9. æµ‹è¯•ç­–ç•¥

#### å•å…ƒæµ‹è¯•
```go
// è¡¨ç®¡ç†æµ‹è¯•
func TestTableCreation(t *testing.T) {
    service := setupTestService()
    
    // æµ‹è¯•åˆ›å»ºè¡¨
    resp, err := service.CreateTable(context.Background(), &CreateTableRequest{
        TableName: "test_table",
        Config: &TableConfig{
            BufferSize: 1000,
            FlushIntervalSeconds: 30,
        },
    })
    
    assert.NoError(t, err)
    assert.True(t, resp.Success)
    
    // éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
    exists, err := service.tableExists("test_table")
    assert.NoError(t, err)
    assert.True(t, exists)
}

// è¡¨çº§å†™å…¥æµ‹è¯•
func TestTableWrite(t *testing.T) {
    service := setupTestService()
    
    // åˆ›å»ºæµ‹è¯•è¡¨
    service.CreateTable(context.Background(), &CreateTableRequest{
        TableName: "test_table",
    })
    
    // å†™å…¥æ•°æ®
    resp, err := service.Write(context.Background(), &WriteRequest{
        Table: "test_table",
        Id: "test-001",
        Timestamp: timestamppb.Now(),
        Payload: &structpb.Struct{
            Fields: map[string]*structpb.Value{
                "name": structpb.NewStringValue("test"),
            },
        },
    })
    
    assert.NoError(t, err)
    assert.True(t, resp.Success)
}
```

#### é›†æˆæµ‹è¯•
```bash
# é›†æˆæµ‹è¯•è„šæœ¬
#!/bin/bash

# å¯åŠ¨æµ‹è¯•ç¯å¢ƒ
docker-compose -f test/docker-compose.test.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 10

# åˆ›å»ºæµ‹è¯•è¡¨
curl -X POST http://localhost:8081/v1/tables \
  -H "Content-Type: application/json" \
  -d '{"table_name": "integration_test", "config": {"buffer_size": 100}}'

# å†™å…¥æµ‹è¯•æ•°æ®
for i in {1..500}; do
  curl -X POST http://localhost:8081/v1/data \
    -H "Content-Type: application/json" \
    -d "{\"table\": \"integration_test\", \"id\": \"test-$i\", \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\", \"payload\": {\"value\": $i}}"
done

# ç­‰å¾…æ•°æ®åˆ·æ–°
sleep 35

# æŸ¥è¯¢æµ‹è¯•
result=$(curl -X POST http://localhost:8081/v1/query \
  -H "Content-Type: application/json" \
  -d '{"sql": "SELECT COUNT(*) as count FROM integration_test"}')

echo "Query result: $result"

# æ¸…ç†æµ‹è¯•ç¯å¢ƒ
docker-compose -f test/docker-compose.test.yml down
```

#### æ€§èƒ½æµ‹è¯•
```bash
# æ€§èƒ½æµ‹è¯•è„šæœ¬
#!/bin/bash

# åˆ›å»ºæ€§èƒ½æµ‹è¯•è¡¨
curl -X POST http://localhost:8081/v1/tables \
  -H "Content-Type: application/json" \
  -d '{"table_name": "perf_test", "config": {"buffer_size": 10000, "flush_interval_seconds": 5}}'

# å¹¶å‘å†™å…¥æµ‹è¯•
echo "Starting concurrent write test..."
for i in {1..10}; do
  (
    for j in {1..1000}; do
      curl -s -X POST http://localhost:8081/v1/data \
        -H "Content-Type: application/json" \
        -d "{\"table\": \"perf_test\", \"id\": \"perf-$i-$j\", \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\", \"payload\": {\"thread\": $i, \"seq\": $j}}" > /dev/null
    done
  ) &
done

wait
echo "Write test completed"

# æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
echo "Starting query performance test..."
time curl -X POST http://localhost:8081/v1/query \
  -H "Content-Type: application/json" \
  -d '{"sql": "SELECT thread, COUNT(*) FROM perf_test GROUP BY thread ORDER BY thread"}'

echo "Performance test completed"
```

## ğŸš€ é¢„æœŸæ”¶ç›Š

### æ€§èƒ½æå‡
- **æŸ¥è¯¢æ€§èƒ½**ï¼šè¡¨çº§åˆ†ç¦»å‡å°‘æ— å…³æ•°æ®æ‰«æï¼ŒæŸ¥è¯¢é€Ÿåº¦æå‡50-80%
- **å†™å…¥æ€§èƒ½**ï¼šè¡¨çº§ç¼“å†²åŒºé…ç½®ä¼˜åŒ–ï¼Œå†™å…¥ååé‡æå‡30-50%
- **å­˜å‚¨æ•ˆç‡**ï¼šæŒ‰è¡¨ç»„ç»‡æ•°æ®ï¼Œå‹ç¼©ç‡æå‡10-20%
- **ç´¢å¼•æ•ˆç‡**ï¼šè¡¨çº§ç´¢å¼•å‡å°‘Redisé”®ç©ºé—´ï¼ŒæŸ¥è¯¢é€Ÿåº¦æå‡40-60%

### ç®¡ç†ä¾¿åˆ©æ€§
- **ä¸šåŠ¡åˆ†ç¦»**ï¼šä¸åŒä¸šåŠ¡æ•°æ®ç‹¬ç«‹ç®¡ç†ï¼Œé™ä½è¿ç»´å¤æ‚åº¦
- **é…ç½®çµæ´»**ï¼šè¡¨çº§é…ç½®æ»¡è¶³ä¸åŒä¸šåŠ¡éœ€æ±‚
- **æƒé™æ§åˆ¶**ï¼šè¡¨çº§æƒé™æå‡æ•°æ®å®‰å…¨æ€§
- **ç›‘æ§ç²¾ç»†**ï¼šè¡¨çº§ç›‘æ§ä¾¿äºé—®é¢˜å®šä½å’Œæ€§èƒ½ä¼˜åŒ–

### æ‰©å±•æ€§
- **æ°´å¹³æ‰©å±•**ï¼šè¡¨å¯ä»¥ç‹¬ç«‹åˆ†å¸ƒåˆ°ä¸åŒèŠ‚ç‚¹
- **åŠŸèƒ½æ‰©å±•**ï¼šä¸ºè¡¨çº§ç‰¹æ€§ï¼ˆåˆ†åŒºã€ç´¢å¼•ç­‰ï¼‰å¥ å®šåŸºç¡€
- **ç”Ÿæ€é›†æˆ**ï¼šæ›´å®¹æ˜“ä¸å…¶ä»–æ•°æ®å·¥å…·é›†æˆ
- **å¤šç§Ÿæˆ·**ï¼šæ”¯æŒå¤šç§Ÿæˆ·åœºæ™¯ä¸‹çš„æ•°æ®éš”ç¦»

### å…¼å®¹æ€§
- **å‘åå…¼å®¹**ï¼šç°æœ‰APIå’Œæ•°æ®æ— ç¼è¿‡æ¸¡
- **æ¸è¿›è¿ç§»**ï¼šæ”¯æŒåˆ†æ‰¹è¿ç§»ï¼Œé™ä½é£é™©
- **é…ç½®å…¼å®¹**ï¼šæ—§é…ç½®è‡ªåŠ¨æ˜ å°„åˆ°æ–°ç»“æ„
- **å·¥å…·å…¼å®¹**ï¼šç°æœ‰è¿ç»´å·¥å…·ç»§ç»­å¯ç”¨

## ğŸ“‹ å®æ–½æ£€æŸ¥æ¸…å•

### å¼€å‘é˜¶æ®µ
- [ ] APIæ¥å£è®¾è®¡å’Œprotobufå®šä¹‰
- [ ] è¡¨ç®¡ç†æœåŠ¡å®ç°
- [ ] ç¼“å†²åŒºç®¡ç†æ”¹é€ 
- [ ] æŸ¥è¯¢å¼•æ“å¢å¼º
- [ ] é…ç½®ç®¡ç†æ‰©å±•
- [ ] æ•°æ®è¿ç§»å·¥å…·å¼€å‘

### æµ‹è¯•é˜¶æ®µ
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°80%+
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•è¾¾åˆ°é¢„æœŸæŒ‡æ ‡
- [ ] å…¼å®¹æ€§æµ‹è¯•é€šè¿‡
- [ ] å‹åŠ›æµ‹è¯•é€šè¿‡

### éƒ¨ç½²é˜¶æ®µ
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ
- [ ] éƒ¨ç½²è„šæœ¬æ›´æ–°
- [ ] ç›‘æ§æŒ‡æ ‡é…ç½®
- [ ] å‘Šè­¦è§„åˆ™è®¾ç½®
- [ ] è¿ç»´å·¥å…·æ›´æ–°

### ä¸Šçº¿é˜¶æ®µ
- [ ] ç°åº¦å‘å¸ƒè®¡åˆ’
- [ ] å›æ»šæ–¹æ¡ˆå‡†å¤‡
- [ ] æ•°æ®è¿ç§»æ‰§è¡Œ
- [ ] æ€§èƒ½ç›‘æ§
- [ ] ç”¨æˆ·åŸ¹è®­

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [APIæ¥å£æ–‡æ¡£](api/README.md)
- [é…ç½®è¯´æ˜æ–‡æ¡£](docs/configuration.md)
- [éƒ¨ç½²æŒ‡å—](deploy/README.md)
- [è¿ç»´æ‰‹å†Œ](docs/operations.md)
- [æ•…éšœæ’é™¤æŒ‡å—](docs/troubleshooting.md)

---

**æ³¨æ„**ï¼šæœ¬è®¾è®¡æ–¹æ¡ˆéœ€è¦æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚å’ŒæŠ€æœ¯ç¯å¢ƒè¿›è¡Œè°ƒæ•´ã€‚å»ºè®®åœ¨å®æ–½å‰è¿›è¡Œå……åˆ†çš„æŠ€æœ¯è¯„å®¡å’Œé£é™©è¯„ä¼°ã€‚